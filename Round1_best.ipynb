{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKVbo2dkwC0t"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu6Jp7v2whgy"
      },
      "source": [
        "%cd drive/MyDrive/109_2/NLP/Final_Report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQa14IifmL_u"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNmpGH3Rl8rj"
      },
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        " \n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r')\n",
        "\ttext = file.read()\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "def clean_doc(doc):\n",
        "\ttokens = doc.split()\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        " \n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\tdoc = load_doc(filename)\n",
        "\ttokens = clean_doc(doc)\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "def process_docs(path, vocab):\n",
        "\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "vocab = Counter()\n",
        "process_docs('txt_sentoken/neg.txt', vocab)\n",
        "process_docs('txt_sentoken/pos.txt', vocab)\n",
        "print(len(vocab))\n",
        "print(vocab.most_common(50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qNQCCghoGu5"
      },
      "source": [
        "def save_list(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "save_list(vocab, 'vocab.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5pBZoz29OMS"
      },
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "def clean_doc(doc, vocab):\n",
        "\ttokens = doc.split()\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\ttokens = ' '.join(tokens)\n",
        "\treturn tokens\n",
        "\n",
        "def process_docs(path, vocab):\n",
        "  documents = list()\n",
        "  file = open(path, 'r')\n",
        "  doc = file.readline()\n",
        "  while doc:\n",
        "    tokens = clean_doc(doc, vocab)\n",
        "    documents.append(tokens)\n",
        "    doc = file.readline()\n",
        "  file.close()\n",
        "  return documents\n",
        "\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "all_positive_docs = process_docs('txt_sentoken/pos.txt', vocab)\n",
        "all_negative_docs = process_docs('txt_sentoken/neg.txt', vocab)\n",
        "positive_docs = all_positive_docs[:2500]\n",
        "negative_docs = all_negative_docs[:2500]\n",
        "train_docs = negative_docs + positive_docs\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define training labels\n",
        "ytrain = array([0 for _ in range(len(negative_docs))] + [1 for _ in range(len(positive_docs))])\n",
        "\n",
        "positive_docs = all_positive_docs[2500:]\n",
        "negative_docs = all_negative_docs[2500:]\n",
        "test_docs = negative_docs + positive_docs\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "ytest = array([0 for _ in range(len(negative_docs))] + [1 for _ in range(len(positive_docs))])\n",
        "\n",
        "def process_docs(path, vocab):\n",
        "  documents = list()\n",
        "  file = open(path, 'r')\n",
        "  doc = file.readline()\n",
        "  while doc:\n",
        "    tokens = clean_doc(doc, vocab)\n",
        "    documents.append(tokens)\n",
        "    doc = file.readline()\n",
        "  file.close()\n",
        "  return documents\n",
        "\n",
        "eval_docs = process_docs('txt_sentoken/eval.txt', vocab)\n",
        "encoded_eval_docs = tokenizer.texts_to_sequences(eval_docs)\n",
        "Xeval = pad_sequences(encoded_eval_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(Xtrain, ytrain, epochs=6, verbose=2)\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rae4_Ax9Eyc"
      },
      "source": [
        "results = (model.predict(Xeval) > 0.5).astype(\"int32\")\n",
        "with open(\"results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  for i in range(len(results)):\n",
        "    if results[i][0] == 0:\n",
        "      f.writelines(\"fake\\n\")\n",
        "    else:\n",
        "      f.writelines(\"real\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}